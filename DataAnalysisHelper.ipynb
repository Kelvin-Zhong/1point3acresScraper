{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "import jieba.analyse\n",
    "from collections import Counter\n",
    "import math\n",
    "import operator\n",
    "%matplotlib inline\n",
    "\n",
    "class DataAnalysisHelper:\n",
    "    def bucketize(self, min_value, max_value, bin_size, values):\n",
    "        bucket_names = []\n",
    "        bucket_values = []\n",
    "        step = (max_value - min_value) / (bin_size)\n",
    "        for i in range(1, bin_size + 1):\n",
    "            cur_min_value = step * (i - 1)\n",
    "            cur_max_value = step * i\n",
    "            cur_bucket_name = str(int(cur_min_value)) + ' - ' + str(int(cur_max_value))\n",
    "            bucket_names.append(cur_bucket_name)\n",
    "\n",
    "            cur_bucket_value = 0\n",
    "            for v in values:\n",
    "                if v >= cur_min_value and v < cur_max_value:\n",
    "                    cur_bucket_value += 1\n",
    "            bucket_values.append(cur_bucket_value)\n",
    "\n",
    "        last_bucket_name = '> ' + str(max_value)\n",
    "        bucket_names.append(last_bucket_name)\n",
    "        last_bucket_value = 0\n",
    "        for v in values:\n",
    "            if v >= max_value:\n",
    "                last_bucket_value += 1\n",
    "        bucket_values.append(last_bucket_value)\n",
    "\n",
    "        return (bucket_names, bucket_values)\n",
    "    \n",
    "    def bisectCommentsForPostData(self, posts_data, postDataFilterFunCase):\n",
    "        num_comments_pass_filter = []\n",
    "        num_comments_fail_filter = []\n",
    "        for i in range(len(posts_data)):\n",
    "            \n",
    "            should_pass_filter = False\n",
    "            if postDataFilterFunCase == 'isTargetedFemale':\n",
    "                should_pass_filter = self.isTargetedFemale(posts_data[i])\n",
    "            elif postDataFilterFunCase == 'hasEmailInText':\n",
    "                should_pass_filter = self.hasEmailInText(posts_data[i])\n",
    "            elif postDataFilterFunCase == 'hasLongText':\n",
    "                should_pass_filter = self.hasLongText(posts_data[i])\n",
    "                \n",
    "            if should_pass_filter:\n",
    "                num_comments_pass_filter.append(posts_data[i]['post_comment_num'])\n",
    "            else:\n",
    "                num_comments_fail_filter.append(posts_data[i]['post_comment_num'])\n",
    "        return (num_comments_pass_filter, num_comments_fail_filter)\n",
    "    \n",
    "    \n",
    "    def drawBarChart(self, bucket_names, bucket_values):\n",
    "        y_pos = np.arange(len(bucket_names))\n",
    "        plt.figure(figsize=(8,5))\n",
    "        plt.bar(y_pos, bucket_values, align='center', alpha=0.5)\n",
    "        plt.xticks(y_pos, bucket_names)\n",
    "        plt.ylim([0, 50])\n",
    "        plt.show()\n",
    "        \n",
    "    \n",
    "    def drawPieChart(self, bucket_names, bucket_values):\n",
    "        plt.figure(figsize=(8,5))\n",
    "        plt.pie(bucket_values, labels=bucket_names, shadow=True, autopct='%1.0f%%', startangle=90)\n",
    "        plt.axis('equal')\n",
    "        plt.show()\n",
    "        \n",
    "    \n",
    "    def getListStatistics(self, values):\n",
    "        return {\n",
    "            'mean': np.mean(values),\n",
    "            'median': np.median(values),\n",
    "        }\n",
    "    \n",
    "    def printAllStats(self, bucket_names, bucket_values):\n",
    "        self.drawBarChart(bucket_names, bucket_values)\n",
    "        self.drawPieChart(bucket_names, bucket_values)\n",
    "        print(data_helper.getListStatistics(bucket_values))\n",
    "    \n",
    "    def isTargetedFemale(self, postData):\n",
    "        if postData['target_gender'] == 'Female':\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def hasEmailInText(self, postData):\n",
    "        match = re.search(r'[\\w\\.-]+@[\\w\\.-]+', postData['message_text'])\n",
    "        if match == None:\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    def hasLongText(self, postData):\n",
    "        if len(postData['message_text']) < 320 :\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    def getMeaninglessWordTags(self):\n",
    "        return ['x', 'm', 'r', 'c', 'ul', 'uj', 'd', 'f', 'p', 'uz', 'eng', 'q', 'zg']\n",
    "    \n",
    "    def BreakWordsAndCleanUp(self, text):\n",
    "        #seg_list = jieba.lcut(text, cut_all=False)\n",
    "        words = pseg.cut(text)\n",
    "        seg_list = []\n",
    "        for word, flag in words:\n",
    "            #print(word, flag)\n",
    "            if flag in self.getMeaninglessWordTags():\n",
    "                continue\n",
    "            seg_list.append(word)\n",
    "        return seg_list\n",
    "    \n",
    "    def filterWordsBasedOnTags(self, words, tags):\n",
    "        filtered_words = []\n",
    "        for word in words:\n",
    "            for tag in tags:\n",
    "                tmp_word = pseg.cut(word)\n",
    "                print(tmp_word)\n",
    "                \n",
    "    def getTopTFIDFWordsForBothGoodAndBadPostDatas(self, posts_data, postCommentThreshold=20):\n",
    "        for i in range(len(posts_data)):\n",
    "            posts_data[i]['text_words'] = self.BreakWordsAndCleanUp(posts_data[i]['message_text'])\n",
    "\n",
    "        postDataIndexWithGoodComments = []\n",
    "        postDataIndexWithBadComments = []\n",
    "        for i in range(len(posts_data)):\n",
    "            if posts_data[i]['post_comment_num'] > postCommentThreshold:\n",
    "                postDataIndexWithGoodComments.append(i)\n",
    "            else:\n",
    "                postDataIndexWithBadComments.append(i)\n",
    "\n",
    "        tfidf = TFIDF(posts_data)\n",
    "        good_comment_post_words = tfidf.getTopWordsInDocuments(postDataIndexWithGoodComments, 40, 100)\n",
    "        bad_comment_post_words = tfidf.getTopWordsInDocuments(postDataIndexWithBadComments, 40, 100)\n",
    "\n",
    "        good_diff_words = copy.deepcopy(good_comment_post_words)\n",
    "        for word in good_comment_post_words:\n",
    "            if word in bad_comment_post_words:\n",
    "                del good_diff_words[word]\n",
    "        good_diff_words = sorted(good_diff_words.items(), key = operator.itemgetter(1), reverse = True)\n",
    "\n",
    "        bad_diff_words = copy.deepcopy(bad_comment_post_words)\n",
    "        for word in bad_comment_post_words:\n",
    "            if word in good_comment_post_words:\n",
    "                del bad_diff_words[word]\n",
    "        bad_diff_words = sorted(bad_diff_words.items(), key = operator.itemgetter(1), reverse = True)\n",
    "\n",
    "        return (good_diff_words, bad_diff_words)\n",
    "    \n",
    "    def wordsDiff(self, a_words, b_words):\n",
    "        a_words = set([x[0] for x in a_words])\n",
    "        b_words = set([x[0] for x in b_words])\n",
    "        diff = a_words - b_words\n",
    "        return diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFIDF:\n",
    "    def __init__(self, postDatas):\n",
    "        num_of_documents = len(postDatas)\n",
    "        documents = [x for x in range(num_of_documents)]\n",
    "        documents_word_counters = []\n",
    "        \n",
    "        # remove words that only appear once\n",
    "        total_words = []\n",
    "        for i in range(num_of_documents):\n",
    "            word_counter = Counter(postDatas[i]['text_words'])\n",
    "            words_in_document = list(word_counter.keys())\n",
    "            for word in words_in_document:\n",
    "                total_words.append(word)\n",
    "        total_word_counter = Counter(total_words)\n",
    "        \n",
    "        for i in range(num_of_documents):\n",
    "            # Compute TF\n",
    "            word_counter = Counter(postDatas[i]['text_words'])\n",
    "            words_in_document = list(word_counter.keys())\n",
    "            total_words_in_document = sum(list(word_counter.values()))\n",
    "            for word in words_in_document:\n",
    "                if total_word_counter[word] > 1:\n",
    "                    word_counter[word] = word_counter[word] / total_words_in_document\n",
    "                else:\n",
    "                    word_counter[word] = 0\n",
    "            \n",
    "            documents_word_counters.append(word_counter)\n",
    "\n",
    "        tfidf_matrix_dict = {}\n",
    "        \n",
    "        for i in range(num_of_documents):\n",
    "            tfidf_matrix_dict[i] = {}\n",
    "            # Compute IDF\n",
    "            for word in list(documents_word_counters[i].keys()):\n",
    "                exist_document = 0\n",
    "                for j in range(num_of_documents):\n",
    "                    if word in documents_word_counters[j]:\n",
    "                        exist_document += 1\n",
    "                idf = math.log(num_of_documents / exist_document, 10)\n",
    "                # Compute TFIDF\n",
    "                tfidf = documents_word_counters[i][word] * idf\n",
    "                tfidf_matrix_dict[i][word] = tfidf\n",
    "        \n",
    "        self.tfidf_matrix_dict = tfidf_matrix_dict\n",
    "            \n",
    "    def getTopWordsInDocument(self, documentIndex, numOfTop):\n",
    "        tfidf_in_document = self.tfidf_matrix_dict[documentIndex]\n",
    "        tfidf_in_document_sorted = sorted(tfidf_in_document.items(), key = operator.itemgetter(1), reverse = True)\n",
    "        tfidf_in_document_sorted = tfidf_in_document_sorted[:numOfTop]\n",
    "        return dict(tfidf_in_document_sorted)\n",
    "    \n",
    "    def getTopWordsInDocuments(self, documentIndexes, numOfTopInDocument, numOfTop):\n",
    "        words_with_score = {}\n",
    "        for index in documentIndexes:\n",
    "            words_in_document = self.getTopWordsInDocument(index, numOfTopInDocument)\n",
    "            for word in list(words_in_document.keys()):\n",
    "                if not word in words_with_score:\n",
    "                    words_with_score[word] = 0\n",
    "                words_with_score[word] += words_in_document[word]\n",
    "        words_with_score_sorted = sorted(words_with_score.items(), key = operator.itemgetter(1), reverse = True)\n",
    "        words_with_score_sorted = words_with_score_sorted[:numOfTop]\n",
    "        return dict(words_with_score_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Case\n",
    "# postDatas = [\n",
    "#     {'text_words': ['this', 'is', 'a', 'sample', 'sample']},\n",
    "#     {'text_words': ['this', 'is', 'another', 'another', 'example', 'example', 'example']},\n",
    "# ]\n",
    "# tfidf = TFIDF(postDatas)\n",
    "# print(tfidf.tfidf_matrix_dict)\n",
    "# print(tfidf.getTopWordsInDocuments([0,1], 1000, 1000))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
